{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9y4yaEFfTjpZLkoGATlzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoPassaro95/Python-Projects/blob/main/Scraper~tool_for_internship_offers_with_STRINGS_and_RE_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraper~tool for internship offers with strings and regular expressions\n",
        "As an aspiring Data Scientist, I am interested in monitoring some Linkedin internships offers just to have an idea on what I am going to apply during next months.\n",
        "###Let's try to do it in a funny automated way!\n",
        "\n"
      ],
      "metadata": {
        "id": "JI-0JpRBFTO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import random as rnd\n",
        "import time\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "faUds6KUzFn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Understanding Linkedin URL for job search\n",
        "By directly formatting URL it is possible to retrieve the offer page without manipulating anything else"
      ],
      "metadata": {
        "id": "G7jhPS27RuZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many offers are there?\n",
        "\n",
        "results_pattern = '<div class=\"jobs-search-results-list__subtitle><span>(.*?)</span>'\n",
        "results_match = re.search(results_pattern, lin_html).group().strip()\n",
        "print(results_match)\n"
      ],
      "metadata": {
        "id": "1OTD1-_azIEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic formatting function for Linkedin URL according to desired queries\n",
        "This function fill Linkedin url with desired queries formatted in the correct way for internships research"
      ],
      "metadata": {
        "id": "cDWPZoHGHlYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def URLformat(keyword='', area='', job_type = ''):\n",
        "\n",
        "  keyword = input(\"Please insert keywords for LinkedIn search: \")\n",
        "  area = input(\"Where would you like to find this offer? \")\n",
        "  job_type = input(\"Insert job type: F for full time, P for part-time, C for contractor, I for stages/internships \")\n",
        "\n",
        "  # Error Message in case of missing query inputs and function immediate recall\n",
        "\n",
        "  if keyword=='' or area=='' or job_type == '':\n",
        "\n",
        "    print('Error: the words you have inserted are not correct, please try again')\n",
        "    return  URLformat()\n",
        "\n",
        "  # Then, replace function is used to fill\n",
        "  # spaces with %20 and commas with %2C%20 thus formatting keyword and location inputs for the URL.\n",
        "  # These replacements are necessary to ensure proper URL encoding\n",
        "\n",
        "  search_url = keyword.replace(\" \", \"%20\")\n",
        "\n",
        "  if \", \" in area:\n",
        "    area_url = area.replace(\", \", \"%2C%20\")\n",
        "  else:\n",
        "    area_url = area.replace(\" \", \"%20\")\n",
        "\n",
        "    # The linkedin_url variable (\"f-string\" form) contains variables of specified keywords\n",
        "    # and location (as query parameters). This URL is so setted to perform a job search on LinkedIn.\n",
        "\n",
        "  linkedin_url =f'https://www.linkedin.com/jobs/search?keywords={search_url}&location={area_url}&f_JT={job_type}&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
        "\n",
        "  return linkedin_url"
      ],
      "metadata": {
        "id": "4vS0Uj6qRHvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_url = URLformat()"
      ],
      "metadata": {
        "id": "mcakH_c3zafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linkedin_url)"
      ],
      "metadata": {
        "id": "ZLeQBN1CzdOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scraping function for offer cards relevant information\n",
        "Once finished its work, the function will return a built in dedicated dataset"
      ],
      "metadata": {
        "id": "Mbx2QTNS3wbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LINternScrape(url):\n",
        "\n",
        "  #1. Get page and check for HTTP correct response\n",
        "  lin_page = requests.get(url)\n",
        "\n",
        "  print(lin_page.status_code,lin_page.headers['content-type'])\n",
        "  if lin_page.status_code != 200:\n",
        "    print(\"ERROR: Unsuccessfull HTTP response! Try again\")\n",
        "    return LINternScrape() # 200 value indicates Successfull response\n",
        "\n",
        "\n",
        "  #2. Store page content in a string variable\n",
        "\n",
        "  lin_html = lin_page.text\n",
        "\n",
        "  #3. Bulding lists of all relevant information with regular expression\n",
        "\n",
        "   #a Datetime\n",
        "  dates = []\n",
        "  dat_pattern ='<time class=\".*?\" datetime=\"(.*?)\">'\n",
        "  datetimes =  re.findall(dat_pattern, lin_html, re.DOTALL)\n",
        "\n",
        "  # re.findall returns a list of tuples containing all the matched objects within html string\n",
        "\n",
        "  for date in datetimes:\n",
        "    dates.append(date)\n",
        "\n",
        "   #b Offer Title\n",
        "  titles = []\n",
        "  offer_title_pattern = '<h3(.*?)>(.*?)</h3>'\n",
        "  offer_titles = re.findall(offer_title_pattern, lin_html, re.DOTALL)\n",
        "\n",
        "  # It is necessary to iterate tuples thus accessing and isolating the second element [1] that corresponds to offer title\n",
        "  # Linkedin pages show 25 offers per time, then a while condition avoids addition of other confusing elements\n",
        "\n",
        "  i = 0\n",
        "  while offer_titles and i <= 24:\n",
        "    offer_title = offer_titles[i][1].strip()  #Access through indexing tuple objects in a list\n",
        "    titles.append(offer_title)\n",
        "    i = i + 1\n",
        "\n",
        "   #c Companies\n",
        "  companies = []\n",
        "\n",
        "  company_pattern = \"<h4(.*?)>(.*?)</h4>\"\n",
        "  offer_companies = re.findall(company_pattern, lin_html, re.DOTALL)\n",
        "\n",
        "  for company_match in offer_companies:\n",
        "    company = company_match[1]\n",
        "    company = re.sub(\"<.*?>\", \"\", company).strip()\n",
        "    companies.append(company)\n",
        "\n",
        "   #d Path to offer page\n",
        "  paths = []\n",
        "\n",
        "  pattern = 'href=\"(https://it.linkedin.com/jobs/view/.*?)\"'\n",
        "  links = re.findall(pattern, lin_html, re.DOTALL)\n",
        "\n",
        "  for path in links:\n",
        "  #No need to manipulate string, just retrieve element\n",
        "    paths.append(path)\n",
        "\n",
        "  #4. Create a data frame of all scraped information\n",
        "\n",
        "  Internships = pd.DataFrame({\n",
        "    'Date': dates,\n",
        "    'Company': companies,\n",
        "    'Title': titles,\n",
        "    'Link': paths\n",
        "})\n",
        "\n",
        "  return Internships"
      ],
      "metadata": {
        "id": "3cT0k0jI9GDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LINternScrape(linkedin_url)"
      ],
      "metadata": {
        "id": "KNyzkZzFOTM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scraping Descriptions from each Internship offer page with Descrape function\n",
        "Once finished all descriptions will be added to Internships dataset"
      ],
      "metadata": {
        "id": "_2hTBatm4Rc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Descrape():\n",
        "\n",
        "  descriptions = []\n",
        "\n",
        "  for link in Internships[\"Link\"]:\n",
        "\n",
        "\n",
        "    #offer_link = Internships[\"Link\"][link]\n",
        "\n",
        "    offer_page = requests.get(link)\n",
        "    print(offer_page.status_code, offer_page.headers['content-type'], \"Description correctly scraped!\")\n",
        "\n",
        "    offer_html = offer_page.text\n",
        "\n",
        "    description_pattern = '<div class=\"show-more-less-html__markup (.*?)>(.*?)</div>'\n",
        "\n",
        "  # re.search() scans all the string looking for the first location where pattern produces the first match returning a \"match object\"\n",
        "\n",
        "    description_match = re.search(description_pattern, offer_html, re.DOTALL).group(2).strip()\n",
        "\n",
        "  # re.sub() to remove all the tags and return only description text.\n",
        "\n",
        "    description = re.sub(\"<.*?>\", \"\", description_match)\n",
        "    descriptions.append(description)\n",
        "\n",
        "  # IMPORTANT : Avoid too many HTTP requests at the same time\n",
        "  # Suspend running code for 6 plus 1 to 3 random seconds simulating human behaviour\n",
        "\n",
        "    time.sleep(5+rnd.randint(1,3))\n",
        "\n",
        "\n",
        "    #print(f'The description of the {Internships[link]} offer link has been retrieved! Please wait 6 and few more seconds for another ')\n",
        "\n",
        "    #print(description)\n",
        "  print(\"All descriptions have been stored, thank you for waiting !\")\n",
        "\n",
        "  # Add scraped description to Internships dataset\n",
        "  Internships['Description'] = descriptions\n",
        "\n",
        "  # Get final Internship dataset result\n",
        "  return Internships\n"
      ],
      "metadata": {
        "id": "us2bYJz94QjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Descrape()"
      ],
      "metadata": {
        "id": "HudLoCDkO_eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In case you are interested in just one or some descriptions after a first look to the dataset"
      ],
      "metadata": {
        "id": "4qHnflTmGi_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OneDescrape(index = None):\n",
        "  index = int(input(\"Please insert position index of the description you want to scrape: \"))\n",
        "\n",
        "  if index > len(Internships):\n",
        "    print(\"ERROR! Please insert a number between 1 and 25 \")\n",
        "    return OneDescrape(index)\n",
        "\n",
        "  descr_page = requests.get(Internships[\"Link\"][index])\n",
        "  descr_html = descr_page.text\n",
        "\n",
        "  descr_pat = '<div class=\"show-more-less-html__markup (.*?)>(.*?)</div>'\n",
        "\n",
        "  # re.search() scans all the string looking for the first location where pattern produces the first match returning a \"match object\"\n",
        "\n",
        "  descr_match = re.search(descr_pat, descr_html, re.DOTALL).group(2).strip()\n",
        "\n",
        "  # re.sub() to remove all the tags and return only description text.\n",
        "\n",
        "  description = re.sub(\"<.*?>\", \"\", descr_match)\n",
        "\n",
        "  return description\n"
      ],
      "metadata": {
        "id": "kHOtprsBHLnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Just looking at an interesting offer description"
      ],
      "metadata": {
        "id": "TzQ0w061QevG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description_name = OneDescrape()\n",
        "description_name"
      ],
      "metadata": {
        "id": "yMTFnVT6QCxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}